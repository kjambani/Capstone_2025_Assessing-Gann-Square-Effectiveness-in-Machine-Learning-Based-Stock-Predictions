{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "-ekuilAMpmSd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "TICKER = \"GOOG\"\n",
        "START_DATE = \"2011-01-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "STATE_WINDOW = 10\n",
        "SWING_WINDOW = 15\n",
        "TRAIN_MONTHS = 6\n",
        "TEST_MONTHS = 1\n",
        "EPISODES = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "GAMMA = 0.95\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "XZ56ZaE3p9IE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TradingEnv:\n",
        "    def __init__(self, data, window=STATE_WINDOW):\n",
        "        self.data = data\n",
        "        self.window = window\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = self.window\n",
        "        self.position = 0\n",
        "        self.entry_price = 0\n",
        "        self.total_profit = 0\n",
        "        self.trades = []\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return np.zeros(self.window * 10 + 1)\n",
        "        features = self.data[['Open','High','Low','Close',\n",
        "                               'Angle_1x1_Dist','Angle_1x1_Above',\n",
        "                               'Angle_2x1_Dist','Angle_2x1_Above',\n",
        "                               'Angle_1x2_Dist','Angle_1x2_Above']] \\\n",
        "                    .iloc[self.current_step - self.window:self.current_step].values\n",
        "        norm = (features - np.mean(features, axis=0)) / (np.std(features, axis=0)+1e-8)\n",
        "        state = norm.flatten()\n",
        "        return np.append(state, self.position)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return self._get_state(), 0.0, True\n",
        "        reward = 0.0\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        transaction_cost = 0.001\n",
        "        if action == 1:\n",
        "            if self.position == 0:\n",
        "                self.position = 1; self.entry_price = current_price; reward = -transaction_cost\n",
        "            elif self.position == -1:\n",
        "                profit = (self.entry_price - current_price)/self.entry_price\n",
        "                reward = profit-transaction_cost; self.total_profit+=reward\n",
        "                self.trades.append(('close_short', profit))\n",
        "                self.position=1; self.entry_price=current_price\n",
        "        elif action == 2:\n",
        "            if self.position == 0:\n",
        "                self.position=-1; self.entry_price=current_price; reward=-transaction_cost\n",
        "            elif self.position == 1:\n",
        "                profit=(current_price-self.entry_price)/self.entry_price\n",
        "                reward=profit-transaction_cost; self.total_profit+=reward\n",
        "                self.trades.append(('close_long', profit))\n",
        "                self.position=-1; self.entry_price=current_price\n",
        "        elif action == 0:\n",
        "            if self.position==1:\n",
        "                reward=((current_price-self.entry_price)/self.entry_price)*0.1\n",
        "            elif self.position==-1:\n",
        "                reward=((self.entry_price-current_price)/self.entry_price)*0.1\n",
        "        self.current_step+=1\n",
        "        done=self.current_step>=len(self.data)\n",
        "        return self._get_state(), float(reward), done\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "# -----------------------------\n",
        "# Replay Buffer\n",
        "# -----------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity): self.memory = deque(maxlen=capacity)\n",
        "    def push(self, s,a,r,ns,d): self.memory.append((s,a,r,ns,d))\n",
        "    def sample(self, batch_size): return random.sample(self.memory, batch_size)\n",
        "    def __len__(self): return len(self.memory)\n",
        "\n",
        "\n",
        "def train_agent(train_data):\n",
        "    state_dim = STATE_WINDOW*10+1; action_dim=3\n",
        "    env=TradingEnv(train_data)\n",
        "    policy_net=DQN(state_dim,action_dim); target_net=DQN(state_dim,action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    optimizer=optim.Adam(policy_net.parameters(),lr=LR)\n",
        "    memory=ReplayBuffer(MEMORY_SIZE)\n",
        "    epsilon=EPSILON_START\n",
        "    for ep in range(EPISODES):\n",
        "        state=env.reset(); ep_reward=0; done=False\n",
        "        while not done:\n",
        "            if random.random()<epsilon:\n",
        "                action=random.randrange(action_dim)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action=torch.argmax(policy_net(torch.tensor(state,dtype=torch.float32))).item()\n",
        "            next_state,reward,done=env.step(action)\n",
        "            memory.push(state,action,reward,next_state,done)\n",
        "            state=next_state; ep_reward+=reward\n",
        "            if len(memory)>=BATCH_SIZE:\n",
        "                batch=memory.sample(BATCH_SIZE)\n",
        "                states,actions,rewards,next_states,dones=zip(*batch)\n",
        "                states=torch.tensor(np.array(states),dtype=torch.float32)\n",
        "                actions=torch.tensor(actions,dtype=torch.int64).unsqueeze(1)\n",
        "                rewards=torch.tensor(rewards,dtype=torch.float32).unsqueeze(1)\n",
        "                next_states=torch.tensor(np.array(next_states),dtype=torch.float32)\n",
        "                dones=torch.tensor(dones,dtype=torch.float32).unsqueeze(1)\n",
        "                q_vals=policy_net(states).gather(1,actions)\n",
        "                next_q=target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "                target=rewards+(GAMMA*next_q*(1-dones))\n",
        "                loss=nn.MSELoss()(q_vals,target)\n",
        "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        epsilon=max(EPSILON_MIN,epsilon*EPSILON_DECAY)\n",
        "        if ep%TARGET_UPDATE==0: target_net.load_state_dict(policy_net.state_dict())\n",
        "        if ep%10==0: print(f\"Episode {ep}/{EPISODES}, Reward:{ep_reward:.4f}, Epsilon:{epsilon:.3f}\")\n",
        "    return policy_net\n",
        "\n",
        "\n",
        "def test_agent(test_data,model):\n",
        "    env=TradingEnv(test_data); state=env.reset(); done=False\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            action=torch.argmax(model(torch.tensor(state,dtype=torch.float32))).item()\n",
        "        state,_,done=env.step(action)\n",
        "    return env.total_profit,len(env.trades)\n",
        "\n",
        "\n",
        "def run_walk_forward_test():\n",
        "    results=[]; start_idx=0; days_per_month=21\n",
        "    train_days=TRAIN_MONTHS*days_per_month; test_days=TEST_MONTHS*days_per_month; period=0\n",
        "    while True:\n",
        "        train_end=start_idx+train_days; test_end=train_end+test_days\n",
        "        if test_end>=len(df): break\n",
        "        period+=1; train_data=df.iloc[start_idx:train_end].copy(); test_data=df.iloc[train_end:test_end].copy()\n",
        "        print(f\"\\n--- Period {period} ---\")\n",
        "        print(f\"Train: {train_data.index[0]} → {train_data.index[-1]}\")\n",
        "        print(f\"Test: {test_data.index[0]} → {test_data.index[-1]}\")\n",
        "        model=train_agent(train_data); profit,trades=test_agent(test_data,model)\n",
        "        print(f\"Test profit: {profit:.4f}, Trades: {trades}\")\n",
        "        results.append({'period':period,'profit':profit,'num_trades':trades,\n",
        "                        'train_start':train_data.index[0],'train_end':train_data.index[-1],\n",
        "                        'test_start':test_data.index[0],'test_end':test_data.index[-1]})\n",
        "        start_idx+=test_days\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "JjubGCNSp2AO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GKUBMCPp152"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkmsBJeChTA5",
        "outputId": "6245a83f-6505-400a-e603-a10bf72461b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (3269, 11)\n",
            "Date range: 2011-01-04 00:00:00 to 2023-12-29 00:00:00\n",
            "Columns: ['Open', 'High', 'Low', 'Close', 'Angle_1x1_Dist', 'Angle_1x1_Above', 'Angle_2x1_Dist', 'Angle_2x1_Above', 'Angle_1x2_Dist', 'Angle_1x2_Above', 'Returns']\n",
            "\n",
            "--- Period 1 ---\n",
            "Train: 2011-01-04 00:00:00 → 2011-07-05 00:00:00\n",
            "Test: 2011-07-06 00:00:00 → 2011-08-03 00:00:00\n",
            "Episode 0/50, Reward:0.0366, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.0148, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.0525, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.0900, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.1757, Epsilon:0.814\n",
            "Test profit: -0.0272, Trades: 1\n",
            "\n",
            "--- Period 2 ---\n",
            "Train: 2011-02-03 00:00:00 → 2011-08-03 00:00:00\n",
            "Test: 2011-08-04 00:00:00 → 2011-09-01 00:00:00\n",
            "Episode 0/50, Reward:-0.1027, Epsilon:0.995\n",
            "Episode 10/50, Reward:0.1020, Epsilon:0.946\n",
            "Episode 20/50, Reward:0.1533, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.3741, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.1298, Epsilon:0.814\n",
            "Test profit: 0.0667, Trades: 3\n",
            "\n",
            "--- Period 3 ---\n",
            "Train: 2011-03-07 00:00:00 → 2011-09-01 00:00:00\n",
            "Test: 2011-09-02 00:00:00 → 2011-10-03 00:00:00\n",
            "Episode 0/50, Reward:-0.1394, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.3733, Epsilon:0.946\n",
            "Episode 20/50, Reward:0.4732, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.1491, Epsilon:0.856\n",
            "Episode 40/50, Reward:-0.0399, Epsilon:0.814\n",
            "Test profit: 0.0613, Trades: 2\n",
            "\n",
            "--- Period 4 ---\n",
            "Train: 2011-04-05 00:00:00 → 2011-10-03 00:00:00\n",
            "Test: 2011-10-04 00:00:00 → 2011-11-01 00:00:00\n",
            "Episode 0/50, Reward:-0.1935, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.1855, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.0421, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.4742, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.1244, Epsilon:0.814\n",
            "Test profit: -0.0333, Trades: 3\n",
            "\n",
            "--- Period 5 ---\n",
            "Train: 2011-05-05 00:00:00 → 2011-11-01 00:00:00\n",
            "Test: 2011-11-02 00:00:00 → 2011-12-01 00:00:00\n",
            "Episode 0/50, Reward:-0.3119, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.0992, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.2423, Epsilon:0.900\n",
            "Episode 30/50, Reward:-0.2477, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.6076, Epsilon:0.814\n",
            "Test profit: 0.0017, Trades: 2\n",
            "\n",
            "--- Period 6 ---\n",
            "Train: 2011-06-06 00:00:00 → 2011-12-01 00:00:00\n",
            "Test: 2011-12-02 00:00:00 → 2012-01-03 00:00:00\n",
            "Episode 0/50, Reward:-0.1453, Epsilon:0.995\n",
            "Episode 10/50, Reward:0.1623, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.5630, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.2277, Epsilon:0.856\n",
            "Episode 40/50, Reward:-0.2161, Epsilon:0.814\n",
            "Test profit: -0.0163, Trades: 2\n",
            "\n",
            "--- Period 7 ---\n",
            "Train: 2011-07-06 00:00:00 → 2012-01-03 00:00:00\n",
            "Test: 2012-01-04 00:00:00 → 2012-02-02 00:00:00\n",
            "Episode 0/50, Reward:0.0233, Epsilon:0.995\n",
            "Episode 10/50, Reward:0.1394, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.0841, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.1626, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.1543, Epsilon:0.814\n",
            "Test profit: -0.0850, Trades: 2\n",
            "\n",
            "--- Period 8 ---\n",
            "Train: 2011-08-04 00:00:00 → 2012-02-02 00:00:00\n",
            "Test: 2012-02-03 00:00:00 → 2012-03-05 00:00:00\n",
            "Episode 0/50, Reward:-0.0850, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.1137, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.0508, Epsilon:0.900\n",
            "Episode 30/50, Reward:0.0878, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.2523, Epsilon:0.814\n",
            "Test profit: -0.0031, Trades: 5\n",
            "\n",
            "--- Period 9 ---\n",
            "Train: 2011-09-02 00:00:00 → 2012-03-05 00:00:00\n",
            "Test: 2012-03-06 00:00:00 → 2012-04-03 00:00:00\n",
            "Episode 0/50, Reward:0.1061, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.1379, Epsilon:0.946\n",
            "Episode 20/50, Reward:0.0681, Epsilon:0.900\n",
            "Episode 30/50, Reward:-0.3568, Epsilon:0.856\n",
            "Episode 40/50, Reward:0.1410, Epsilon:0.814\n",
            "Test profit: 0.0013, Trades: 1\n",
            "\n",
            "--- Period 10 ---\n",
            "Train: 2011-10-04 00:00:00 → 2012-04-03 00:00:00\n",
            "Test: 2012-04-04 00:00:00 → 2012-05-03 00:00:00\n",
            "Episode 0/50, Reward:-0.1972, Epsilon:0.995\n",
            "Episode 10/50, Reward:-0.4246, Epsilon:0.946\n",
            "Episode 20/50, Reward:-0.2872, Epsilon:0.900\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def find_swing_points(df, window):\n",
        "    swing_highs, swing_lows = [], []\n",
        "    for i in range(window, len(df)-window):\n",
        "        high = df['High'].iloc[i]\n",
        "        low = df['Low'].iloc[i]\n",
        "        if high == df['High'].iloc[i-window:i+window+1].max():\n",
        "            swing_highs.append((df.index[i], high))\n",
        "        if low == df['Low'].iloc[i-window:i+window+1].min():\n",
        "            swing_lows.append((df.index[i], low))\n",
        "    return swing_highs, swing_lows\n",
        "\n",
        "\n",
        "def assign_gann_angle_features(df, swing_highs, swing_lows):\n",
        "    df = df.copy()\n",
        "    angles = {\"1x1\": 1.0, \"2x1\": 2.0, \"1x2\": 0.5}\n",
        "\n",
        "    for name in angles.keys():\n",
        "        df[f\"Angle_{name}_Dist\"] = np.nan\n",
        "        df[f\"Angle_{name}_Above\"] = np.nan\n",
        "\n",
        "    pivots = sorted(swing_highs + swing_lows, key=lambda x: x[0])\n",
        "\n",
        "    for i, (date, price) in enumerate(pivots):\n",
        "        direction = 1 if (date, price) in swing_lows else -1\n",
        "        if i < len(pivots) - 1:\n",
        "            next_date = pivots[i+1][0]\n",
        "            mask = (df.index >= date) & (df.index < next_date)\n",
        "        else:\n",
        "            mask = (df.index >= date)\n",
        "\n",
        "        days = np.arange(mask.sum())\n",
        "        for name, slope in angles.items():\n",
        "            projected_prices = price + direction * slope * days\n",
        "            actual_prices = df.loc[mask, 'Close'].values\n",
        "            dist = actual_prices - projected_prices\n",
        "            above = (actual_prices > projected_prices).astype(int)\n",
        "            df.loc[mask, f\"Angle_{name}_Dist\"] = dist\n",
        "            df.loc[mask, f\"Angle_{name}_Above\"] = above\n",
        "\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"Downloading data...\")\n",
        "df = yf.download(TICKER, start=START_DATE, end=END_DATE)\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = [col[0] for col in df.columns]\n",
        "df = df[['Open', 'High', 'Low', 'Close']].copy()\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "swing_highs, swing_lows = find_swing_points(df, SWING_WINDOW)\n",
        "df = assign_gann_angle_features(df, swing_highs, swing_lows)\n",
        "df['Returns'] = df['Close'].pct_change()\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "results=run_walk_forward_test()\n",
        "\n",
        "print(\"WALK-FORWARD TEST RESULTS (Gann Angles)\");\n",
        "\n",
        "total_profit=sum([r['profit'] for r in results]); total_trades=sum([r['num_trades'] for r in results])\n",
        "for r in results: print(f\"Period {r['period']}: Profit={r['profit']:.4f}, Trades={r['num_trades']}\")\n",
        "profits=[r['profit'] for r in results]\n",
        "print(f\"\\nSUMMARY:\\nTotal periods:{len(results)}\\nTotal profit:{total_profit:.4f}\\n\"\n",
        "      f\"Average profit:{np.mean(profits):.4f}\\nTotal trades:{total_trades}\\n\")\n",
        "\n"
      ]
    }
  ]
}