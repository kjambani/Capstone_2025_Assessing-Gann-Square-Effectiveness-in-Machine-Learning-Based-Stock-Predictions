{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "9KtkwiR-mwEP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TICKER = \"GOOG\"\n",
        "START_DATE = \"2011-01-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "STATE_WINDOW = 10\n",
        "TRAIN_MONTHS = 6\n",
        "TEST_MONTHS = 1\n",
        "EPISODES = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "GAMMA = 0.95\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 10"
      ],
      "metadata": {
        "id": "MnBi-9yTnSAt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv:\n",
        "    def __init__(self, data, window=STATE_WINDOW):\n",
        "        self.data = data\n",
        "        self.window = window\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = self.window\n",
        "        self.position = 0  # 0 = flat, 1 = long, -1 = short\n",
        "        self.entry_price = 0.0\n",
        "        self.total_profit = 0.0\n",
        "        self.trades = []\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return np.zeros(self.window * 6 + 1)  # 6 features: OHLC + 2 Gann\n",
        "\n",
        "        features = self.data[['Open', 'High', 'Low', 'Close',\n",
        "                               'GannRes_rel', 'GannSup_rel']] \\\n",
        "                    .iloc[self.current_step - self.window:self.current_step].values\n",
        "\n",
        "        # Normalize features (z-score per window)\n",
        "        norm = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)\n",
        "        state = norm.flatten()\n",
        "        state = np.append(state, self.position)\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return self._get_state(), 0.0, True\n",
        "\n",
        "        reward = 0.0\n",
        "        current_price = float(self.data.iloc[self.current_step]['Close'])\n",
        "        transaction_cost = 0.001\n",
        "\n",
        "        # Action 0: Hold, Action 1: Buy, Action 2: Sell\n",
        "        if action == 1:  # Buy\n",
        "            if self.position == 0:  # Enter long\n",
        "                self.position = 1\n",
        "                self.entry_price = current_price\n",
        "                reward = -transaction_cost\n",
        "            elif self.position == -1:  # Close short, enter long\n",
        "                profit = (self.entry_price - current_price) / self.entry_price\n",
        "                reward = profit - transaction_cost\n",
        "                self.total_profit += reward\n",
        "                self.trades.append(('close_short', profit))\n",
        "                self.position = 1\n",
        "                self.entry_price = current_price\n",
        "\n",
        "        elif action == 2:  # Sell\n",
        "            if self.position == 0:  # Enter short\n",
        "                self.position = -1\n",
        "                self.entry_price = current_price\n",
        "                reward = -transaction_cost\n",
        "            elif self.position == 1:  # Close long, enter short\n",
        "                profit = (current_price - self.entry_price) / self.entry_price\n",
        "                reward = profit - transaction_cost\n",
        "                self.total_profit += reward\n",
        "                self.trades.append(('close_long', profit))\n",
        "                self.position = -1\n",
        "                self.entry_price = current_price\n",
        "\n",
        "        elif action == 0:  # Hold\n",
        "            if self.position == 1:  # Long position\n",
        "                unrealized_pnl = (current_price - self.entry_price) / self.entry_price\n",
        "                reward = unrealized_pnl * 0.1  # Small reward for unrealized gains\n",
        "            elif self.position == -1:  # Short position\n",
        "                unrealized_pnl = (self.entry_price - current_price) / self.entry_price\n",
        "                reward = unrealized_pnl * 0.1  # Small reward for unrealized gains\n",
        "            else:\n",
        "                reward = 0.0  # No position, no reward\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "\n",
        "        # Close position if episode is done\n",
        "        if done and self.position != 0:\n",
        "            if self.position == 1:\n",
        "                final_profit = (current_price - self.entry_price) / self.entry_price\n",
        "                self.trades.append(('final_close_long', final_profit))\n",
        "            else:\n",
        "                final_profit = (self.entry_price - current_price) / self.entry_price\n",
        "                self.trades.append(('final_close_short', final_profit))\n",
        "            self.total_profit += final_profit\n",
        "\n",
        "        return self._get_state(), float(reward), done\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, output_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "def train_agent(train_data):\n",
        "    state_dim = STATE_WINDOW * 6 + 1  # 6 features (OHLC + 2 Gann) + position\n",
        "    action_dim = 3\n",
        "    env = TradingEnv(train_data)\n",
        "\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    target_net = DQN(state_dim, action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    memory = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "    epsilon = EPSILON_START\n",
        "    episode_rewards = []\n",
        "\n",
        "    print(\"Training agent...\")\n",
        "    for ep in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randrange(action_dim)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "                    q_values = policy_net(state_tensor)\n",
        "                    action = torch.argmax(q_values).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Train the network\n",
        "            if len(memory) >= BATCH_SIZE:\n",
        "                batch = memory.sample(BATCH_SIZE)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                # Convert to tensors with proper handling\n",
        "                states = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "                actions = torch.tensor(np.array(actions), dtype=torch.int64).unsqueeze(1)\n",
        "                rewards = torch.tensor(np.array(rewards, dtype=np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "                next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "                dones = torch.tensor(np.array(dones, dtype=np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                # Q-learning update\n",
        "                current_q_values = policy_net(states).gather(1, actions)\n",
        "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "                target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
        "\n",
        "                # Compute loss and update\n",
        "                loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)  # Gradient clipping\n",
        "                optimizer.step()\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
        "\n",
        "        # Update target network\n",
        "        if ep % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"Episode {ep}/{EPISODES}, Reward: {episode_reward:.4f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    return policy_net, episode_rewards\n",
        "\n",
        "\n",
        "def test_agent(test_data, trained_model):\n",
        "    env = TradingEnv(test_data)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    actions_taken = []\n",
        "\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            q_values = trained_model(state_tensor)\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "        actions_taken.append(action)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        state = next_state\n",
        "\n",
        "    # Ensure scalar return values\n",
        "    total_profit = float(env.total_profit)\n",
        "    num_trades = int(len(env.trades))\n",
        "\n",
        "    return total_profit, num_trades, actions_taken\n",
        "\n",
        "\n",
        "def run_walk_forward_test():\n",
        "    results = []\n",
        "    start_idx = 0\n",
        "    period = 0\n",
        "\n",
        "    days_per_month = 21\n",
        "    train_days = TRAIN_MONTHS * days_per_month\n",
        "    test_days = TEST_MONTHS * days_per_month\n",
        "\n",
        "    print(f\"Train period: {TRAIN_MONTHS} months ({train_days} days)\")\n",
        "    print(f\"Test period: {TEST_MONTHS} months ({test_days} days)\")\n",
        "\n",
        "    while True:\n",
        "        train_end_idx = start_idx + train_days\n",
        "        test_end_idx = train_end_idx + test_days\n",
        "\n",
        "        if test_end_idx >= len(df):\n",
        "            break\n",
        "\n",
        "        period += 1\n",
        "        train_data = df.iloc[start_idx:train_end_idx].copy()\n",
        "        test_data = df.iloc[train_end_idx:test_end_idx].copy()\n",
        "\n",
        "        print(f\"\\n--- Period {period} ---\")\n",
        "        print(f\"Train: {train_data.index[0].strftime('%Y-%m-%d')} to {train_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Test: {test_data.index[0].strftime('%Y-%m-%d')} to {test_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        model, training_rewards = train_agent(train_data)\n",
        "        profit, num_trades, actions = test_agent(test_data, model)\n",
        "\n",
        "        results.append({\n",
        "            'period': period,\n",
        "            'profit': profit,\n",
        "            'num_trades': num_trades,\n",
        "            'train_start': train_data.index[0],\n",
        "            'train_end': train_data.index[-1],\n",
        "            'test_start': test_data.index[0],\n",
        "            'test_end': test_data.index[-1]\n",
        "        })\n",
        "\n",
        "        print(f\"Test profit: {profit:.4f}\")\n",
        "        print(f\"Number of trades: {num_trades}\")\n",
        "\n",
        "        # Move to next period\n",
        "        start_idx += test_days\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "grAm1mnhnK6J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbipj1YBj9_x",
        "outputId": "25afe27a-020f-488c-b05d-16002e64be29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (3269, 9)\n",
            "Date range: 2011-01-04 00:00:00 to 2023-12-29 00:00:00\n",
            "Column types:\n",
            " Open           float64\n",
            "High           float64\n",
            "Low            float64\n",
            "Close          float64\n",
            "GannRes        float64\n",
            "GannSup        float64\n",
            "GannRes_rel    float64\n",
            "GannSup_rel    float64\n",
            "Returns        float64\n",
            "dtype: object\n",
            "Train period: 6 months (126 days)\n",
            "Test period: 1 months (21 days)\n",
            "\n",
            "--- Period 1 ---\n",
            "Train: 2011-01-04 to 2011-07-05\n",
            "Test: 2011-07-06 to 2011-08-03\n",
            "Training agent...\n",
            "Episode 0/50, Reward: 0.0366, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.0166, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.0518, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.0694, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.1857, Epsilon: 0.814\n",
            "Test profit: 0.0789\n",
            "Number of trades: 2\n",
            "\n",
            "--- Period 2 ---\n",
            "Train: 2011-02-03 to 2011-08-03\n",
            "Test: 2011-08-04 to 2011-09-01\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1027, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.0672, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.1272, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.3644, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.0645, Epsilon: 0.814\n",
            "Test profit: -0.0101\n",
            "Number of trades: 3\n",
            "\n",
            "--- Period 3 ---\n",
            "Train: 2011-03-07 to 2011-09-01\n",
            "Test: 2011-09-02 to 2011-10-03\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1394, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.3715, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.4750, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.1350, Epsilon: 0.856\n",
            "Episode 40/50, Reward: -0.0472, Epsilon: 0.814\n",
            "Test profit: 0.0406\n",
            "Number of trades: 5\n",
            "\n",
            "--- Period 4 ---\n",
            "Train: 2011-04-05 to 2011-10-03\n",
            "Test: 2011-10-04 to 2011-11-01\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1935, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.0731, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.0101, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.1966, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.4716, Epsilon: 0.814\n",
            "Test profit: -0.0102\n",
            "Number of trades: 4\n",
            "\n",
            "--- Period 5 ---\n",
            "Train: 2011-05-05 to 2011-11-01\n",
            "Test: 2011-11-02 to 2011-12-01\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.3119, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.0057, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.3279, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.1153, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.3141, Epsilon: 0.814\n",
            "Test profit: -0.1448\n",
            "Number of trades: 2\n",
            "\n",
            "--- Period 6 ---\n",
            "Train: 2011-06-06 to 2011-12-01\n",
            "Test: 2011-12-02 to 2012-01-03\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1453, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.2374, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.5064, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.0032, Epsilon: 0.856\n",
            "Episode 40/50, Reward: -0.2199, Epsilon: 0.814\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def gann_levels(price: float):\n",
        "\n",
        "    if price <= 0:\n",
        "        return price, price\n",
        "    root = math.sqrt(price)\n",
        "    resistance = (root + 0.25) ** 2\n",
        "    support = max(0, (root - 0.25) ** 2)  # Ensure non-negative\n",
        "    return resistance, support\n",
        "\n",
        "df = yf.download(TICKER, start=START_DATE, end=END_DATE)\n",
        "\n",
        "\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = [col[0] for col in df.columns]\n",
        "\n",
        "df = df[['Open', 'High', 'Low', 'Close']].copy()\n",
        "\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "gann_res, gann_sup = [], []\n",
        "for price in df['Close']:\n",
        "    res, sup = gann_levels(float(price))\n",
        "    gann_res.append(res)\n",
        "    gann_sup.append(sup)\n",
        "\n",
        "df['GannRes'] = gann_res\n",
        "df['GannSup'] = gann_sup\n",
        "\n",
        "# Relative distances to current price\n",
        "df['GannRes_rel'] = (df['GannRes'] - df['Close']) / df['Close']\n",
        "df['GannSup_rel'] = (df['GannSup'] - df['Close']) / df['Close']\n",
        "\n",
        "# Returns for optional metrics\n",
        "df['Returns'] = df['Close'].pct_change()\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "print(\"Column types:\\n\", df.dtypes)\n",
        "\n",
        "\n",
        "\n",
        "results = run_walk_forward_test()\n",
        "\n",
        "# Print summary results\n",
        "print(\"Gann Square with OHLC\")\n",
        "\n",
        "total_profit = 0\n",
        "total_trades = 0\n",
        "\n",
        "for result in results:\n",
        "  print(f\"Period {result['period']}: Profit = {result['profit']:.4f}, Trades = {result['num_trades']}\")\n",
        "  total_profit += result['profit']\n",
        "  total_trades += result['num_trades']\n",
        "\n",
        "print(f\"\\nSUMMARY:\")\n",
        "print(f\"Total periods: {len(results)}\")\n",
        "print(f\"Total profit: {total_profit:.4f}\")\n",
        "print(f\"Average profit per period: {total_profit/len(results):.4f}\")\n",
        "print(f\"Total trades: {total_trades}\")\n",
        "print(f\"Average trades per period: {total_trades/len(results):.1f}\")\n",
        "\n",
        "# Calculate basic statistics\n",
        "profits = [r['profit'] for r in results]\n",
        "win_rate = len([p for p in profits if p > 0]) / len(profits) * 100\n"
      ]
    }
  ]
}