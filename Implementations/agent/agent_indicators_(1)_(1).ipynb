{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6J7sqvO_0EP",
        "outputId": "89351629-ab46-4b1a-8313-020c86f3f298"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=7acd61bd7d7c139d0a583719188946b13a0299e1d9833a69995b53b79b86bb7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import ta\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ],
      "metadata": {
        "id": "VoZC-SOroonn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TICKER = \"GOOG\"\n",
        "START_DATE = \"2011-01-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "STATE_WINDOW = 10\n",
        "TRAIN_MONTHS = 6\n",
        "TEST_MONTHS = 1\n",
        "EPISODES = 50\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "GAMMA = 0.95\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "MEMORY_SIZE = 10000\n",
        "TARGET_UPDATE = 10\n"
      ],
      "metadata": {
        "id": "U3l8lgOoosf1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv:\n",
        "    def __init__(self, data, window=STATE_WINDOW):\n",
        "        self.data = data\n",
        "        self.window = window\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = self.window\n",
        "        self.position = 0           # 0 flat, 1 long, -1 short\n",
        "        self.entry_price = 0.0\n",
        "        self.total_profit = 0.0\n",
        "        self.trades = []\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return np.zeros(self.window * 7 + 1)  # 4 OHLC + 3 indicators\n",
        "        features = self.data[['Open', 'High', 'Low', 'Close',\n",
        "                               'RSI', 'MACD', 'MACD_signal']] \\\n",
        "                    .iloc[self.current_step - self.window:self.current_step].values\n",
        "        norm = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-8)\n",
        "        state = norm.flatten()\n",
        "        return np.append(state, self.position)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= len(self.data):\n",
        "            return self._get_state(), 0.0, True\n",
        "        reward = 0.0\n",
        "        current_price = float(self.data.iloc[self.current_step]['Close'])\n",
        "        transaction_cost = 0.001\n",
        "\n",
        "        # 0 Hold, 1 Buy, 2 Sell\n",
        "        if action == 1:\n",
        "            if self.position == 0:\n",
        "                self.position = 1; self.entry_price = current_price\n",
        "                reward = -transaction_cost\n",
        "            elif self.position == -1:\n",
        "                profit = (self.entry_price - current_price) / self.entry_price\n",
        "                reward = profit - transaction_cost\n",
        "                self.total_profit += reward\n",
        "                self.trades.append(('close_short', profit))\n",
        "                self.position = 1; self.entry_price = current_price\n",
        "        elif action == 2:\n",
        "            if self.position == 0:\n",
        "                self.position = -1; self.entry_price = current_price\n",
        "                reward = -transaction_cost\n",
        "            elif self.position == 1:\n",
        "                profit = (current_price - self.entry_price) / self.entry_price\n",
        "                reward = profit - transaction_cost\n",
        "                self.total_profit += reward\n",
        "                self.trades.append(('close_long', profit))\n",
        "                self.position = -1; self.entry_price = current_price\n",
        "        elif action == 0:\n",
        "            if self.position == 1:\n",
        "                reward = ((current_price - self.entry_price) / self.entry_price) * 0.1\n",
        "            elif self.position == -1:\n",
        "                reward = ((self.entry_price - current_price) / self.entry_price) * 0.1\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "        # (Optional close at end is handled in other scripts; we leave consistent behavior here)\n",
        "        return self._get_state(), float(reward), done\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def train_agent(train_data):\n",
        "    state_dim = STATE_WINDOW * 7 + 1  # 7 features + position\n",
        "    action_dim = 3\n",
        "    env = TradingEnv(train_data)\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    target_net = DQN(state_dim, action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    memory = ReplayBuffer(MEMORY_SIZE)\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    print(\"Training agent...\")\n",
        "    for ep in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randrange(action_dim)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(policy_net(torch.tensor(state, dtype=torch.float32))).item()\n",
        "            next_state, reward, done = env.step(action)\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(memory) >= BATCH_SIZE:\n",
        "                states, actions, rewards, next_states, dones = zip(*memory.sample(BATCH_SIZE))\n",
        "                states = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
        "                next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "                q_values = policy_net(states).gather(1, actions)\n",
        "                next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "                target = rewards + (GAMMA * next_q_values * (1 - dones))\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "        epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
        "        if ep % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"Episode {ep}/{EPISODES}, Reward: {episode_reward:.4f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "\n",
        "def test_agent(test_data, trained_model):\n",
        "    env = TradingEnv(test_data)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            action = torch.argmax(trained_model(torch.tensor(state, dtype=torch.float32))).item()\n",
        "        next_state, reward, done = env.step(action)\n",
        "        state = next_state\n",
        "    return float(env.total_profit), len(env.trades)\n",
        "\n",
        "\n",
        "def run_walk_forward_test():\n",
        "    results = []\n",
        "    start_idx = 0\n",
        "    days_per_month = 21\n",
        "    train_days = TRAIN_MONTHS * days_per_month\n",
        "    test_days = TEST_MONTHS * days_per_month\n",
        "    period = 0\n",
        "\n",
        "    print(f\"\\nStarting walk-forward test (RSI + MACD)...\")\n",
        "    print(f\"Train period: {TRAIN_MONTHS} months ({train_days} days)\")\n",
        "    print(f\"Test period: {TEST_MONTHS} months ({test_days} days)\")\n",
        "\n",
        "    while True:\n",
        "        train_end_idx = start_idx + train_days\n",
        "        test_end_idx = train_end_idx + test_days\n",
        "        if test_end_idx >= len(df):\n",
        "            break\n",
        "        period += 1\n",
        "        train_data = df.iloc[start_idx:train_end_idx].copy()\n",
        "        test_data = df.iloc[train_end_idx:test_end_idx].copy()\n",
        "\n",
        "        print(f\"\\n--- Period {period} ---\")\n",
        "        print(f\"Train: {train_data.index[0].strftime('%Y-%m-%d')} to {train_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"Test:  {test_data.index[0].strftime('%Y-%m-%d')} to {test_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        model = train_agent(train_data)\n",
        "        profit, trades = test_agent(test_data, model)\n",
        "\n",
        "        print(f\"Test profit: {profit:.4f}\")\n",
        "        print(f\"Number of trades: {trades}\")\n",
        "\n",
        "        results.append({\n",
        "            'period': period,\n",
        "            'profit': profit,\n",
        "            'num_trades': trades,\n",
        "            'train_start': train_data.index[0],\n",
        "            'train_end': train_data.index[-1],\n",
        "            'test_start': test_data.index[0],\n",
        "            'test_end': test_data.index[-1]\n",
        "        })\n",
        "\n",
        "        start_idx += test_days\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "M0FR9y1-oytr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkcUVUvU_rmb",
        "outputId": "12fa0b13-eb71-4fd1-c00f-63d8bba70380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data...\n",
            "Data shape: (3236, 8)\n",
            "Date range: 2011-02-22 00:00:00 to 2023-12-29 00:00:00\n",
            "Columns: ['Open', 'High', 'Low', 'Close', 'RSI', 'MACD', 'MACD_signal', 'Returns']\n",
            "\n",
            "Starting walk-forward test (RSI + MACD)...\n",
            "Train period: 6 months (126 days)\n",
            "Test period: 1 months (21 days)\n",
            "\n",
            "--- Period 1 ---\n",
            "Train: 2011-02-22 to 2011-08-19\n",
            "Test:  2011-08-22 to 2011-09-20\n",
            "Training agent...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/50, Reward: -0.1862, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.3691, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.1736, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.0373, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.3241, Epsilon: 0.814\n",
            "Test profit: -0.0436\n",
            "Number of trades: 5\n",
            "\n",
            "--- Period 2 ---\n",
            "Train: 2011-03-23 to 2011-09-20\n",
            "Test:  2011-09-21 to 2011-10-19\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.5397, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.1411, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.0148, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.0395, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.1187, Epsilon: 0.814\n",
            "Test profit: 0.1066\n",
            "Number of trades: 1\n",
            "\n",
            "--- Period 3 ---\n",
            "Train: 2011-04-21 to 2011-10-19\n",
            "Test:  2011-10-20 to 2011-11-17\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.0713, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.3751, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.0269, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.6624, Epsilon: 0.856\n",
            "Episode 40/50, Reward: -0.2433, Epsilon: 0.814\n",
            "Test profit: 0.0207\n",
            "Number of trades: 6\n",
            "\n",
            "--- Period 4 ---\n",
            "Train: 2011-05-23 to 2011-11-17\n",
            "Test:  2011-11-18 to 2011-12-19\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1809, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.1999, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.3973, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.0122, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.2484, Epsilon: 0.814\n",
            "Test profit: 0.0084\n",
            "Number of trades: 1\n",
            "\n",
            "--- Period 5 ---\n",
            "Train: 2011-06-22 to 2011-12-19\n",
            "Test:  2011-12-20 to 2012-01-20\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.5405, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.2367, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.0056, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.5833, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.3671, Epsilon: 0.814\n",
            "Test profit: 0.0000\n",
            "Number of trades: 0\n",
            "\n",
            "--- Period 6 ---\n",
            "Train: 2011-07-22 to 2012-01-20\n",
            "Test:  2012-01-23 to 2012-02-21\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.4523, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.3204, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.1740, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.0656, Epsilon: 0.856\n",
            "Episode 40/50, Reward: -0.1372, Epsilon: 0.814\n",
            "Test profit: -0.0209\n",
            "Number of trades: 7\n",
            "\n",
            "--- Period 7 ---\n",
            "Train: 2011-08-22 to 2012-02-21\n",
            "Test:  2012-02-22 to 2012-03-21\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.2516, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.0149, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.2523, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.2186, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.1286, Epsilon: 0.814\n",
            "Test profit: 0.0477\n",
            "Number of trades: 7\n",
            "\n",
            "--- Period 8 ---\n",
            "Train: 2011-09-21 to 2012-03-21\n",
            "Test:  2012-03-22 to 2012-04-20\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.5734, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.1150, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.3020, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.1868, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.1757, Epsilon: 0.814\n",
            "Test profit: 0.0333\n",
            "Number of trades: 2\n",
            "\n",
            "--- Period 9 ---\n",
            "Train: 2011-10-20 to 2012-04-20\n",
            "Test:  2012-04-23 to 2012-05-21\n",
            "Training agent...\n",
            "Episode 0/50, Reward: 0.1659, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.4544, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.1482, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.1577, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.4327, Epsilon: 0.814\n",
            "Test profit: 0.0315\n",
            "Number of trades: 1\n",
            "\n",
            "--- Period 10 ---\n",
            "Train: 2011-11-18 to 2012-05-21\n",
            "Test:  2012-05-22 to 2012-06-20\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.2455, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.0465, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.2627, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.2769, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.2011, Epsilon: 0.814\n",
            "Test profit: 0.0436\n",
            "Number of trades: 4\n",
            "\n",
            "--- Period 11 ---\n",
            "Train: 2011-12-20 to 2012-06-20\n",
            "Test:  2012-06-21 to 2012-07-20\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.0200, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.0388, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.0412, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.0939, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.2887, Epsilon: 0.814\n",
            "Test profit: -0.0468\n",
            "Number of trades: 2\n",
            "\n",
            "--- Period 12 ---\n",
            "Train: 2012-01-23 to 2012-07-20\n",
            "Test:  2012-07-23 to 2012-08-20\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.3723, Epsilon: 0.995\n",
            "Episode 10/50, Reward: 0.0569, Epsilon: 0.946\n",
            "Episode 20/50, Reward: 0.0218, Epsilon: 0.900\n",
            "Episode 30/50, Reward: 0.1050, Epsilon: 0.856\n",
            "Episode 40/50, Reward: -0.0626, Epsilon: 0.814\n",
            "Test profit: 0.0000\n",
            "Number of trades: 0\n",
            "\n",
            "--- Period 13 ---\n",
            "Train: 2012-02-22 to 2012-08-20\n",
            "Test:  2012-08-21 to 2012-09-19\n",
            "Training agent...\n",
            "Episode 0/50, Reward: -0.1262, Epsilon: 0.995\n",
            "Episode 10/50, Reward: -0.2543, Epsilon: 0.946\n",
            "Episode 20/50, Reward: -0.1849, Epsilon: 0.900\n",
            "Episode 30/50, Reward: -0.1158, Epsilon: 0.856\n",
            "Episode 40/50, Reward: 0.4863, Epsilon: 0.814\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Downloading data...\")\n",
        "df = yf.download(TICKER, start=START_DATE, end=END_DATE)\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = [col[0] for col in df.columns]\n",
        "df = df[['Open', 'High', 'Low', 'Close']].copy()\n",
        "\n",
        "# Technical Indicators\n",
        "df['RSI'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()\n",
        "macd = ta.trend.MACD(df['Close'])\n",
        "df['MACD'] = macd.macd()\n",
        "df['MACD_signal'] = macd.macd_signal()\n",
        "\n",
        "# Clean\n",
        "for col in df.columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "df['Returns'] = df['Close'].pct_change()\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "results = run_walk_forward_test()\n",
        "\n",
        "\n",
        "print(\"WALK-FORWARD TEST RESULTS (RSI + MACD)\")\n",
        "\n",
        "profits = [r['profit'] for r in results]\n",
        "total_profit = float(np.sum(profits))\n",
        "total_trades = int(np.sum([r['num_trades'] for r in results]))\n",
        "for r in results:\n",
        "    print(f\"Period {r['period']}: Profit = {r['profit']:.4f}, Trades = {r['num_trades']}\")\n",
        "print(f\"\\nSUMMARY:\")\n",
        "print(f\"Total periods: {len(results)}\")\n",
        "print(f\"Total profit: {total_profit:.4f}\")\n",
        "print(f\"Average profit per period: {float(np.mean(profits)):.4f}\")\n",
        "print(f\"Total trades: {total_trades}\")\n",
        "print(f\"Win rate: {len([p for p in profits if p > 0]) / len(profits) * 100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mUBilMonKdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}